\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel} 
\title{Combining Word Embeddings with Evolutionary Information to Predict Protein Secondary Structure}
\author{Anna Reithmeir - Bachelorarbeit WS18/19}
\bibliographystyle{ieeetr}

\usepackage{url}

\begin{document}
\maketitle
\begin{abstract}
Abstract goes here
\end{abstract}
\section{Overview}

Proteins secondary structure can be divided into 3 or 8 classes - DSSP classification. 'turn' and 'bridge', repeating turns are 'helices', repeating bridges are 'ladders'. Connected ladders are 'sheets'. There is a relation of the amino acid sequence to the secondary structure of course. H=H(4-turn helix)G(3.turn helix)I(5-turn helix), E=E(ladder)B(bridge), C=S(bend)T(turn)C(coil)\\

PDB: Protein Data Bank, lists of atomic coordinates located through X-ray cristallography, data is sometimes incomplete and subjective.\\

UniProt: Universal Protein Resource. UniParc database contains most of the known protein sequences\\

Å (ångstrom): unit of length, 0.1 nanometre. Used to express sizes of atoms and molecules. Resolution of the protein acid models, 1 Å is high resolution, 3Å low. \\

The data I am working on: sequences from UniProt, corresponding structures taken from PDB where resolution higher than 2.5 Å.\\

What I will do: Encode each protein sequence as a 20xlength one-hot matrix where each position indicates which protein can be found here. Look at the neighbors of each protein and feed each position including it's neighbors - like 'QWP' - to an NN - that would be a 20x3 matrix.The NN should classify the input into 3 or 8 structure classes. But instead we want to use ProtVec and thus have an input size of 3x100 for each 3-sequence. The targets are 3-sequences of structures like 'HHH'. After that I will include the proteon evolution into the input data, by sequence alignment methods. Rost idea: first consider evolution, then combine it with ProtVec later? \\

ProtVec: Table where each possible 3-sequence corresponds to a vector of length 100. Trained on 546 790 smaples of the Swiss-Prot database to encode biophysical and biochemical properties. NLP used. Each word represented as n-dim Vector, close words in terms of semantics and syntax should have close vectors. Skip-gram modelProtein family classification by using neighboring words as context, specifically an overlapping sliding window of 3 amino acids are seen as one word are used to train an NN on maximizing over the probability of observed word sentences. As an output each 3-gram can be identified by an 100 vector. These vectors are then projected onto two dimensions through Stochastic Neighbor embedding. Properties tested were: mass, volume, polarity, hydrophobicity, charge, Van-der-Waals volume. Eventhough the model is trained on sequences only, the authors obtain an weighted accuracy of 93+-0.06\% which is better than the existing methods for protein family classification. It can distinguish between various classes of sequences and is a dense representation of the input protein sequences. Was able to identify disordered sequences with nearly 100\% accuracy. We will consider these vectors as 'pre-training data' from our input sequences.
Skip-gram model: Used to learn high-quality vector representaion of unstructured words. NLP model. The resulting vectors can then be used for simple vector calculations which yield the expected results.

Skip-gram model:\\

The NN: CNN and/or LSTM intuitively. More than one hidden layer because some 3-sequences like 'HEH' cannot be and the hidden layer then pools the sequence like it would have been 'HHH'. Each sequence end cannot be looked at because one neighbor missing. Thus insert a 0-vector there or explore what else could be done here.\\

Data statistics: number of class occurences and locations, avg length, length of distribution.\\

Overall structure:\\

\begin{tabular}{| l | p{10cm} |}
\hline
section 1 & Introduction: Proteins, secondary structure, structure prediction, NLP, NN, My work\\ \hline
section 2 & Related Work on structure prediction with ML \\ \hline
section 3 & The dataset and plots (maybe also classification of biophysical properties\\ \hline
section 4 & Incorporating evolution and ProtVec\\ \hline
section 5 & \\ \hline
section 6 & My network and code overview\\ \hline
section 7 & Outcome, accuracy, plots of input and prediction data\\ \hline
section 8 &	Discussion and problems\\ \hline
section 9 & Outlook\\ \hline
\end{tabular}

\section{Introduction}

\section{Related work}

\section{The dataset}
The data set I will be using for training is taken from the PDB - the RCSB Protein Data Bank \cite{pdb} which holds more than 1 TB of structure data of proteins, DNA and RNA and is considered the leading global resource for protein data in research. The secondary structure is determined through X-ray cristallography which is expensive and sometimes incomplete. It is described through a list of 3d coordinates. To have accurate input data, I will only consider proteins of PDB which are presented with a resolution better than 2.5Å - 3Å is already considered a high resolution.
On the one hand I willuse the amino acid sequences as an input. On the other hand I will use sequences of secondary structure as the training targets, where each structure is assigned to one residue of the amino acid sequence. To obtain these the DSSP algorithm was used, which was developed in 1983 by Kabsch and Sander. This algorithm takes raw 3d coordinates of the atoms of a protein as an input and classifies them either into 3 or 8 classes of structures.
In total I have 3313 protein sequences and their according structures available. 

First I will extract some statistical data of the dataset like the number of occurences of each structure and the average length of a chain of one structure. Then I will encode the available data into a 1-hot vector of the size 20xsequnce length.

\section{Protein evolution and ProtVec}
The key feature of my work is that I will incorporate the envolution of the protein sequences into the input data.

I will use ProtVec \cite{ProtVec}. ProtVec protein vectors represent one protein sequence by a n-dimensional vector, here a vector of length 100, which encodes information on different biochemical and biophysical features. ProtVec was determined by learning represenations in an unsupervised fashion on 546 790 sequencesand adapting methods from existing NLP (Natural Language Processing) methods. Each sequence is split into 'words' of three residues that is into one residue and it'scontext of left and right neighbors. This method is adapted from the Skip-gram model but instead of using n-grams directly for feature extraction they used it for a distributed representation of sequences, attempting to maximize the probability of observed word sequences resulting in a list of 3-grams and their representation as a 100x1 vector. It shows a weighted average accuracy of 93 +- 0.06\% in protein family classification where each sequence is represented as the summation of the representation vectors for each overlapping 3-gram. By down projection of the representation vectors onto a 2 dimensional space via stochastic neighbor embedding the authors show that the vector hold significant information about the distribution of properties like mass, volume, hydrophobicity or charge.ProtVec only needs to be trained once and the resulkting vector representations can then be used as a pre-trained input for mahcine learning tasks, which is what I will do in the following.


\section{My work}
\section{Results}
\section{Discussion}
\section{Outlook}
\bibliography{bib}
\end{document}